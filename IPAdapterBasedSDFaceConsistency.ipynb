{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqbWxeTTc58c62bqv9D7Gv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishalPrem1994/AIGenPlayGround/blob/main/IPAdapterBasedSDFaceConsistency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WR87xEiSBDR",
        "outputId": "26888aa7-eeb6-4928-e3a8-6aacdbd5fc7f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai numpy opencv-python accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers diffusers omegaconf requests\n",
        "%pip install opencv-python insightface onnxruntime\n",
        "%pip install git+https://github.com/tencent-ailab/IP-Adapter.git einops"
      ],
      "metadata": {
        "id": "LW_mRCwFVL3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwbemQenM179",
        "outputId": "89acdcb3-edb0-418e-eb72-c800b365d27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n"
          ]
        }
      ],
      "source": [
        "import insightface\n",
        "import cv2\n",
        "from insightface.app import FaceAnalysis\n",
        "import torch\n",
        "\n",
        "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "app.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "\n",
        "images = [\"/content/drive/MyDrive/stable_diffusion_weights/samples/1.jpg\", \"/content/drive/MyDrive/stable_diffusion_weights/samples/2.jpg\", \"/content/drive/MyDrive/stable_diffusion_weights/samples/3.jpg\", \"/content/drive/MyDrive/stable_diffusion_weights/samples/4.jpg\", \"/content/drive/MyDrive/stable_diffusion_weights/samples/5.jpg\"]\n",
        "\n",
        "### For Face Id Portrait\n",
        "faceid_embeds = []\n",
        "for image in images:\n",
        "    image = cv2.imread(image)\n",
        "    faces = app.get(image)\n",
        "    faceid_embeds.append(torch.from_numpy(faces[0].normed_embedding).unsqueeze(0).unsqueeze(0))\n",
        "faceid_embeds = torch.cat(faceid_embeds, dim=1)\n",
        "\n",
        "### For Face Id\n",
        "image = cv2.imread(\"/content/drive/MyDrive/stable_diffusion_weights/samples/1.jpg\")\n",
        "faces = app.get(image)\n",
        "faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
        "from PIL import Image\n",
        "\n",
        "from ip_adapter.ip_adapter_faceid_separate import IPAdapterFaceID\n",
        "\n",
        "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
        "ip_ckpt = \"/content/drive/MyDrive/stable_diffusion_weights/ip-adapter-faceid_sd15.bin\"\n",
        "device = \"cuda\"\n",
        "\n",
        "noise_scheduler = DDIMScheduler(\n",
        "    num_train_timesteps=1000,\n",
        "    beta_start=0.00085,\n",
        "    beta_end=0.012,\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    clip_sample=False,\n",
        "    set_alpha_to_one=False,\n",
        "    steps_offset=1,\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"/content/drive/MyDrive/stable_diffusion_weights/Mark3\",\n",
        "    torch_dtype=torch.float16,\n",
        "    scheduler=noise_scheduler,\n",
        "    vae=vae,\n",
        "    feature_extractor=None,\n",
        "    safety_checker=None\n",
        ")\n",
        "\n",
        "### For Face Id Portrait\n",
        "ip_model = IPAdapterFaceID(pipe, ip_ckpt, device, num_tokens=16, n_cond=5)\n",
        "\n",
        "### For Face Id\n",
        "ip_model = IPAdapterFaceID(pipe, ip_ckpt, device)"
      ],
      "metadata": {
        "id": "kj76W_D6PevW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate image\n",
        "prompt = \"beautiful indian girl, black hair, at night, strong jawline,  intricate, cinematic composition, dramatic lighting, cinematic view, taken by nina masic\" #@param {type:\"string\"}\n",
        "negative_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\" #@param {type:\"string\"}\n",
        "seed  = 23432344 #@param {type:\"number\"}\n",
        "width=768 #@param {type:\"number\"}\n",
        "height=768 #@param {type:\"number\"}\n",
        "images = ip_model.generate(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    faceid_embeds=faceid_embeds,\n",
        "    num_samples=2,\n",
        "    width=width,\n",
        "    height=height,\n",
        "    num_inference_steps=25,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "uL2NhwzPdALF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}